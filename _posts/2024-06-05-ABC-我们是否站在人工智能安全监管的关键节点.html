---
layout: post
title: 我们是否站在人工智能安全监管的关键节点？
date: 2024-06-05 15:19:03.000000000 +08:00
link: https://newsapp.abc.net.au/newsapp//chinese/2024-06-05/ai-safety-regulation-eu-act-us-china-seoul-summit-concerns/103926850
categories: abc
---

<div><i>2024-06-05T07:11:33.000Z</i></div><header><div data-component="FeatureMedia"><div><div data-component="AspectRatioContainer"><img alt="人工智能机器人" sizes="100vw" src="https://live-production.wcms.abc-cdn.net.au/82a38e139b34d0818411e670621531a0?impolicy=wcms_crop_resize&amp;cropH=1080&amp;cropW=1920&amp;xPos=0&amp;yPos=0&amp;width=862&amp;height=485" loading="lazy" data-component="Image" data-lazy="true"/></div></div><div><small style="color:#999"> <!-- -->二十多位业界学界领军人物呼吁管理人工智能极端风险。</small></div></div></header><div data-component="LayoutContainer"><div><div><div><div><p>在首尔峰会对人工智能的安全承诺一浪高过一浪、欧盟首部《人工智能法》历时三年终将于本月生效之际，20多位人工智能领军人物直指人工智能安全存在严重隐忧，呼吁管理人工智能极端风险。</p><p>二十多位来自美国、中国、英国、加拿大和以色列的重量级教授及学者近日在《科学》（Science）期刊联名<a href="https://www.science.org/doi/10.1126/science.adn0117" data-component="Link">撰文</a>《在快速进步中管理人工智能极端风险》，直指人工智能领域重发展轻安全的现状，呼吁立即调整方向，有效监管，远离灾难。</p><p>这份文章的作者包括人工智能“教父”杰弗里·辛顿（Geoffrey Hinton）、图灵奖得主约书亚·本吉奥（Yoshua Bengio）、姚期智（Andrew Yao）、素有“计算机安全教母“之称的宋晓东（Dawn Song）和《人类简史》三部曲作者尤瓦尔·赫拉利（Yuval Harari）等。</p><p>他们在文章中指出，人工智能正在黑客攻击、社会操纵和战略规划等关键领域取得进展，并可能很快带来前所未有的控制挑战。一旦自主人工智能系统开始追求人类不想要的目标，人类就可能无法控制它们。</p><p>“需要警惕的是，与人类相似，高级人工智能系统可能会在[人类对其的]评估中故意表现不同，假装与人类期望一致，因此这种纯粹的行为评估可能无法奏效，”文章一针见血地指出。</p><p>澳大利亚联邦政府人工智能专家组成员、墨尔本大学人工智能与数字伦理中心创始联合主任珍妮·帕特森教授（Jeannie M Paterson）指出，不假思索地使用人工智能会带来许多风险。</p><div data-component="EmphasisedText"><p>“这些风险又回到了我们自欺欺人的问题上，如果我们把人工智能仅仅看作是一种统计工具的话，尽管它是一种非常复杂的工具，”她说。</p></div><img alt="专家们警告，高级人工智能系统会对人类伪装其表现。" sizes="(max-width: 543px) 543px," srcSet="https://live-production.wcms.abc-cdn.net.au/675ee29c777fbdccb76c7d20cb8ef255?impolicy=wcms_crop_resize&amp;cropH=1414&amp;cropW=1885&amp;xPos=0&amp;yPos=0&amp;width=862&amp;height=647 543w, https://live-production.wcms.abc-cdn.net.au/675ee29c777fbdccb76c7d20cb8ef255?impolicy=wcms_crop_resize&amp;cropH=1414&amp;cropW=2121&amp;xPos=0&amp;yPos=0&amp;width=862&amp;height=575" src="https://live-production.wcms.abc-cdn.net.au/675ee29c777fbdccb76c7d20cb8ef255?impolicy=wcms_crop_resize&amp;cropH=1414&amp;cropW=2121&amp;xPos=0&amp;yPos=0&amp;width=862&amp;height=575" loading="lazy" data-component="Image" data-lazy="true"/><div><div><small style="color:#999"> <!-- -->专家们警告，高级人工智能系统会对人类伪装其表现。</small></div></div><div><button aria-label="Show caption" data-component="Button" aria-expanded="false" type="button"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" width="1em" height="1em" data-component="Info"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="2"></circle><path stroke="currentColor" stroke-width="2" d="M12 7v2m0 2v6"></path></svg></button></div><p>新加坡国立大学教育创新副校长、新加坡国家人工智能核心项目人工智能管理高级总监、《我们，机器人？人工智能监管及其法律局限》一书作者陈西文教授（Simon Chesterman）认为，当前正值管理人工智能的黄金时代，但各方言行却背道而驰。</p><p>“有关人工智能安全性的辩论已迅速从理论走向实践，尤其是对生成式人工智能带来何种影响的担忧，”他说。</p><p>根据斯坦福大学<a href="https://aiindex.stanford.edu/report/" data-component="Link">《2024人工智能指数报告》</a>，美国摇摇领先于中国、欧盟和英国，是世界顶级人工智能模型的主要来源。2023年，全球有61个著名的人工智能模型来自美国的机构，远远超过欧盟的21个和中国的15个。</p><p>生成式人工智能投资也呈井喷式激增。2023年，生成式人工智能的资金比2022年增长了近八倍，达到252亿美元。</p><p>但人工智能安全研究则严重滞后，只有大约1%到3%的人工智能出版物是关于安全方面的。</p><p>报告显示，各国开始重视对人工智能的监管。法律中包含人工智能一词的国家数量大幅增加，从 2022 年的 25 个国家增加到 2023 年的 127 个国家。</p><p>但是，由于各国政府出台的围绕人工智能的法律和指引都在人工智能系统的潜在风险和错失这些新技术带来的经济社会效益的风险中左右平衡，全球对人工智能的安全监管可谓雷声大、雨点小。</p><p>其中，处于人工智能发展最前端的美国、欧盟和中国对安全监管的态度尤为值得关注。</p><h2 data-component="Heading"><strong>欧盟 vs 美国 vs 中国</strong></h2><img alt="欧盟、美国、中国对人工智能的不同监管模式。" sizes="(max-width: 543px) 543px," srcSet="https://live-production.wcms.abc-cdn.net.au/ea139bed6a600661f4d0929d500481ac?impolicy=wcms_crop_resize&amp;cropH=600&amp;cropW=800&amp;xPos=0&amp;yPos=0&amp;width=862&amp;height=647 543w, https://live-production.wcms.abc-cdn.net.au/ea139bed6a600661f4d0929d500481ac?impolicy=wcms_crop_resize&amp;cropH=533&amp;cropW=800&amp;xPos=0&amp;yPos=33&amp;width=862&amp;height=575" src="https://live-production.wcms.abc-cdn.net.au/ea139bed6a600661f4d0929d500481ac?impolicy=wcms_crop_resize&amp;cropH=533&amp;cropW=800&amp;xPos=0&amp;yPos=33&amp;width=862&amp;height=575" loading="lazy" data-component="Image" data-lazy="true"/><div><div><small style="color:#999"> <!-- -->欧盟、美国、中国对人工智能采取不同监管模式。<span data-component="Byline"><span data-component="Text">(<span>Flickr/ News Video</span>)</span></span></small></div></div><div><button aria-label="Show caption" data-component="Button" aria-expanded="false" type="button"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" width="1em" height="1em" data-component="Info"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="2"></circle><path stroke="currentColor" stroke-width="2" d="M12 7v2m0 2v6"></path></svg></button></div><p>本月，世界上首部由主要监管机构制定的关于人工智能的全面法规——欧盟《人工智能法》在历时三年后终于将正式生效。</p><p>根据欧盟网站介绍，这部《人工智能法》将人工智能的应用划分为三个风险类别。首先，造成不可接受风险的应用和系统将被禁止，如中国使用的由政府管理的社会评分类型。其次，高风险应用，如对求职者进行排名的简历扫描工具，须遵守特定的法律要求。最后，未被明确禁止或列为高风险的应用程序基本上不受监管。</p><p>欧盟的这部《人工智能法》采用的是金字塔式风险监管，对高风险人工智能系统实施全生命周期风险管控，被广泛视为迄今为止最严厉管理人工智能的法案。</p><p>“欧盟对防范人工智能偏见和有风险的使用案例有着更普遍的担忧，这促使欧盟通过全面立法来保护公民权利，“陈西文表示。</p><p>与此相比，美国尽管颁布了多项人工智能法案——从《2020年国家人工智能倡议法案》到2023年《人工智能权利法案蓝图》和两个指导国会未来立法的政策框架——但更多强调权利和机遇，鼓励人工智能技术创新发展，采取的是审慎监管原则和联邦与州双线监管问责机制，出台的法案不具约束性。</p><p>“欧盟、美国、英国和中国都采用了不同的监管模式来应对人工智能的危害风险。欧盟的《人工智能法》干预力度最大…以法律监管体系为基础，其中包括全面的隐私保护和人权保护，这使其与干预较少的美国截然不同，”墨尔本大学的帕特森教授说。</p><p>“值得注意的是，欧盟还制定了《数字服务和数字平台法》，对大型数字平台进行特别监管，”帕特森说。</p><img alt="澳大利亚联邦政府人工智能专家组成员、墨尔本大学人工智能与数字伦理中心创始联合主任珍妮·帕特森教授（Jeannie M Paterson）" sizes="(max-width: 543px) 543px," srcSet=", https://live-production.wcms.abc-cdn.net.au/ae58fcb5ef694cbaec1cba77c5b60a06?impolicy=wcms_crop_resize&amp;cropH=361&amp;cropW=541&amp;xPos=0&amp;yPos=21&amp;width=862&amp;height=575" src="https://live-production.wcms.abc-cdn.net.au/ae58fcb5ef694cbaec1cba77c5b60a06?impolicy=wcms_crop_resize&amp;cropH=361&amp;cropW=541&amp;xPos=0&amp;yPos=21&amp;width=862&amp;height=575" loading="lazy" data-component="Image" data-lazy="true"/><div><div><small style="color:#999"> <!-- -->澳大利亚联邦政府人工智能专家组成员、墨尔本大学人工智能与数字伦理中心创始联合主任珍妮·帕特森教授（Jeannie M Paterson）<span data-component="Byline"><span data-component="Text">(<span>Supplied</span>)</span></span></small></div></div><div><button aria-label="Show caption" data-component="Button" aria-expanded="false" type="button"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" width="1em" height="1em" data-component="Info"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="2"></circle><path stroke="currentColor" stroke-width="2" d="M12 7v2m0 2v6"></path></svg></button></div><p>斯坦福大学报告称，2016年至2022年间，美国14 个州通过了与人工智能相关的立法，其中马里兰州居首，通过了七项法案，加利福尼亚州随后，通过了六项法案。</p><p>“美国采取的是一种市场方法，立法很少，不过随着风险变得越来越大，这种情况可能会发生变化，”新加坡国立大学副校长陈西文教授说。</p><p>中国和欧盟一样对人工智能的安全推出了立法监管，但至今未出台一部全面的人工智能法案。</p><p>一度被许多人<a href="https://www.technologyreview.com/2024/01/17/1086704/china-ai-regulation-changes-2024/" data-component="Link">视为</a>全球人工智能监管的领跑者，中国在ChatGPT聊天机器人大放异彩仅几个月后就引入了对生成式人工智能的法律法规，但也因此招来质疑，担心中国政府的做法会阻碍创新。</p><p>从2021年中国发布《新一代人工智能伦理规范》将伦理道德融入人工智能全生命周期，到之后陆续发布《互联网信息服务算法推荐管理规定》、《互联网信息服务深度合成管理规定》和《生成式人工智能服务管理暂行办法》，从算法、深度伪造、生成式人工智能等方面对相关技术的发布者提出监管要求，中国对新技术的反应一直表现得敏捷迅速。目前外界普遍观望，中国是否会效仿欧盟出台全面的《人工智能法》。</p><p>帕特森认为，透明度和专业人才是监管有效的关键。</p><p>“对我来说，任何监管制度的关键要求都是透明，这是监督和管理的必要前提。我们还需要资源充足、具有相关领域专业知识的监管者。否则，法律将沦为一纸空谈，并可能扼杀有益的创新，”她说。</p><p>陈西文指出，人工智能安全监管是一把双刃剑。</p><p>“对大多数国家来说，面临的挑战一方面要避免监管不足，以免让公民面临风险；另一方面又要避免监管过度，避免有可能约束创新或令技术公司另投他处。”</p><p>澳大利亚同样面临这一监管难题。</p><p>今年一月，堪培拉发布了《澳大利亚人工智能安全与责任磋商中期报告》，次月<a href="https://www.minister.industry.gov.au/ministers/husic/media-releases/new-artificial-intelligence-expert-group" data-component="Link">宣布</a>成立人工智能专家组，专门负责向政府提供眼下有关透明度、测试和问责制方面工作的建议，包括在高风险环境中设置对人工智能防护栏的备选方案，以确保人工智能系统的安全。</p><p>作为专家组成员的帕特森教授透露，澳大利亚政府正就更多借鉴欧盟模式还是美国模式进行咨询。</p><div data-component="EmphasisedText"><p>“澳大利亚需要在众多可能的人工智能管理方法中找到自己的道路，考虑到我们现有的法律制度、价值观和地理位置，”她说。</p></div><h2 data-component="Heading"><strong>科技公司内部监管是否可行？</strong></h2><img alt="OpenAI首席执行官山姆·奥特曼出席美国参议院听证会。" sizes="(max-width: 543px) 543px," srcSet="https://live-production.wcms.abc-cdn.net.au/751145f258534c2c8f9f06b0a986ca6e?impolicy=wcms_crop_resize&amp;cropH=2441&amp;cropW=3255&amp;xPos=108&amp;yPos=0&amp;width=862&amp;height=647 543w, https://live-production.wcms.abc-cdn.net.au/751145f258534c2c8f9f06b0a986ca6e?impolicy=wcms_crop_resize&amp;cropH=2020&amp;cropW=3030&amp;xPos=309&amp;yPos=421&amp;width=862&amp;height=575" src="https://live-production.wcms.abc-cdn.net.au/751145f258534c2c8f9f06b0a986ca6e?impolicy=wcms_crop_resize&amp;cropH=2020&amp;cropW=3030&amp;xPos=309&amp;yPos=421&amp;width=862&amp;height=575" loading="lazy" data-component="Image" data-lazy="true"/><div><div><small style="color:#999"> <!-- -->OpenAI首席执行官山姆·奥特曼出席美国参议院听证会。<span data-component="Byline"><span data-component="Text">(<span>Reuters: Elizabeth Frantz</span>)</span></span></small></div></div><div><button aria-label="Show caption" data-component="Button" aria-expanded="false" type="button"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" width="1em" height="1em" data-component="Info"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="2"></circle><path stroke="currentColor" stroke-width="2" d="M12 7v2m0 2v6"></path></svg></button></div><p>随着人工智能基础研究日益迅猛地从各所大学向科技企业转移，管理人工智能的权力正逐渐从公共部门流失到私人领域。科技公司内部如何实施对人工智能的监管备受外界关注。但前景并不乐观。</p><p>六个月前OpenAI发生政变，主张重发展的首席执行官山姆·奥特曼（Sam Altman）突然遭董事会罢免，掀开了该人工智能公司内部理念严重分裂的冰山一角。短短六天后，奥特曼在微软的支持下重返OpenAI头把交椅，并换血一半董事会成员。谁曾想这才是上半集。</p><p>两周前，被视为这一政变推手的OpenAI联合创始人、主张重安全的首席科学家伊利亚·苏茨克沃（Ilya Sutskever）宣布离职。另一只靴子终于落地。硅谷公司在追逐商业利益和公共利益之间不可避免的摩擦似乎以主张重安全派的失败和主张重发展派的胜利而告终。</p><p><a href="https://www.bloomberg.com/news/articles/2024-05-17/openai-dissolves-key-safety-team-after-chief-scientist-ilya-sutskever-s-exit" data-component="Link">据报</a>，苏茨克沃及其副手离职后，OpenAI就实际解散了一个专注于人工智能系统安全的团队，将该团队整合到公司的其他研究工作中。</p><p>本周，十多位OpenAI及谷歌DeepMind 的现任和前任员工发表<a href="https://righttowarn.ai/" data-component="Link">公开信</a>，呼吁重视人工智能技术带来的严重风险，包括导致人类灭绝的风险。</p><p>“人工智能公司有强烈的经济动机来规避有效监督，当前公司治理结构不足以改变这种状况，”公开信说。</p><p>新加坡国立大学副校长陈西文指出，相当一部分人工智能开发人员显然认为，他们的工作确实存在对人类生存构成威胁的风险。但功能更强大的应用程序仍在以更快的速度推出，而负责安全的团队正在被缩减规模或被边缘化，以便能够将这些人工智能产品推向市场。</p><div data-component="EmphasisedText"><p>“在企业中，安全与利润的较量，显然利润通常会胜出，”陈西文说。</p></div><img alt="新加坡国立大学教育创新副校长、新加坡国家人工智能核心项目人工智能管理高级总监、《我们，机器人？人工智能监管及其法律局限》一书作者陈西文教授（Simon Chesterman）" sizes="(max-width: 543px) 543px," srcSet="https://live-production.wcms.abc-cdn.net.au/2869cd7609ae943f51e4ec4f4243bb81?impolicy=wcms_crop_resize&amp;cropH=565&amp;cropW=753&amp;xPos=0&amp;yPos=15&amp;width=862&amp;height=647 543w, https://live-production.wcms.abc-cdn.net.au/2869cd7609ae943f51e4ec4f4243bb81?impolicy=wcms_crop_resize&amp;cropH=502&amp;cropW=753&amp;xPos=0&amp;yPos=30&amp;width=862&amp;height=575" src="https://live-production.wcms.abc-cdn.net.au/2869cd7609ae943f51e4ec4f4243bb81?impolicy=wcms_crop_resize&amp;cropH=502&amp;cropW=753&amp;xPos=0&amp;yPos=30&amp;width=862&amp;height=575" loading="lazy" data-component="Image" data-lazy="true"/><div><div><small style="color:#999"> <!-- -->新加坡国立大学教育创新副校长、新加坡国家人工智能核心项目人工智能管理高级总监、《我们，机器人？人工智能监管及其法律局限》一书作者陈西文教授（Simon Chesterman）<span data-component="Byline"><span data-component="Text">(<span>Supplied</span>)</span></span></small></div></div><div><button aria-label="Show caption" data-component="Button" aria-expanded="false" type="button"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" width="1em" height="1em" data-component="Info"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="2"></circle><path stroke="currentColor" stroke-width="2" d="M12 7v2m0 2v6"></path></svg></button></div><p>包括多位图灵奖获得者在内的20多位人工智能领军人物和著名学者在《科学》期刊撰文呼吁，为了在法规完善之前争取时间，大型人工智能公司应承诺：如果在其人工智能系统中发现触犯红线能力，他们将采取具体安全措施。</p><p>文章指出，这些承诺应该详细，并接受独立审查，监管机构应鼓励各公司争先恐后，利用同类最佳承诺和其他投入，制定适用于所有参与者的标准。</p><p>“影响力与对管理的兴趣之间形成了一种不幸的反比关系，”陈西文说。</p><div data-component="EmphasisedText"><p>“那些拥有最大影响力的公司对管理人工智能的兴趣最小。而那些最感兴趣的组织——非政府组织、联合国等——则最没有影响力去做任何事情。”</p></div><h2 data-component="Heading"><strong>大国角力下的合作与竞争</strong></h2><img alt="美国和中国在人工智能领域展开竞争。" sizes="(max-width: 543px) 543px," srcSet="https://live-production.wcms.abc-cdn.net.au/8b1f0c55ad7dc8f95bb929f87bff2d92?impolicy=wcms_crop_resize&amp;cropH=2410&amp;cropW=3213&amp;xPos=393&amp;yPos=0&amp;width=862&amp;height=647 543w, https://live-production.wcms.abc-cdn.net.au/8b1f0c55ad7dc8f95bb929f87bff2d92?impolicy=wcms_crop_resize&amp;cropH=2410&amp;cropW=3615&amp;xPos=193&amp;yPos=0&amp;width=862&amp;height=575" src="https://live-production.wcms.abc-cdn.net.au/8b1f0c55ad7dc8f95bb929f87bff2d92?impolicy=wcms_crop_resize&amp;cropH=2410&amp;cropW=3615&amp;xPos=193&amp;yPos=0&amp;width=862&amp;height=575" loading="lazy" data-component="Image" data-lazy="true"/><div><div><small style="color:#999"> <!-- -->美国和中国在人工智能领域展开竞争。<span data-component="Byline"><span data-component="Text">(<span>Reuters: Dado Ruvic</span>)</span></span></small></div></div><div><button aria-label="Show caption" data-component="Button" aria-expanded="false" type="button"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" width="1em" height="1em" data-component="Info"><circle cx="12" cy="12" r="9" stroke="currentColor" stroke-width="2"></circle><path stroke="currentColor" stroke-width="2" d="M12 7v2m0 2v6"></path></svg></button></div><p>十六家处于人工智能发展前沿的公司在两周前英国和韩国共同主办的首尔峰会上承诺，在监管机构争相跟进快速创新和新兴风险之际，将安全地发展人工智能技术。</p><p>这些公司包括：美国领军企业谷歌、脸书的母公司Meta、微软、OpenAI、亚马逊、IBM，中国人工智能领跑企业智谱、阿里巴巴、腾讯、小米、美团，以及欧洲、中东和韩国的一些人工智能公司。</p><p>这一《前沿人工智能安全承诺》得到的评价褒贬不一。一些业界人士对这一承诺表示欢迎，另一些人士则批评该承诺纯属空谈，关键要看实际行动。</p><p>除了科技公司联合发布的这项安全承诺外，为期两天的首尔人工智能峰会还烈火烹油般接连发布了五项协议和声明，包括由澳大利亚、美国、英国、新加坡、日本、韩国、加拿大在内的十个国家和欧盟签署的《首尔宣言》，呼吁在人工智能安全、创新和包容性方面开展国际合作。</p><p>“这些国家在打击破坏民主的虚假信息方面有着共同利益。因此我们需要他们取得进展。它们都有兴趣这样做，”墨尔本大学的帕特森教授说。</p><p>但和中国受邀出席去年在英国举行的首届全球人工智能安全峰会并成为《布莱切利宣言》签署国之一迥然不同的是，今年中国并未出现在首尔峰会的领袖会议上，十国和欧盟共同签署的《首尔宣言》，中国也不在其列。这令外界质疑政治竞争和大国角力是否会阻碍人工智能安全监管的全球合作。</p><p>“这是一个潜在的共同利益的悲剧，如果每个人都只从自己的短期利益出发，那么致力于民主这一共同利益就会因为对网上阅读到或看到的内容极度缺乏信任而遭到破坏，”帕特森说。</p><p>在另一份27国和欧盟签署的《首尔部长级声明》中，中国也不在其列，但签署国包括了智利、印度、印尼、肯尼亚、墨西哥、尼日利亚、菲律宾、卢旺达等发展中国家。</p><p>墨尔本大学的帕特森教授表示，发展中国家在人工智能领域的利益不应被忽视。</p><div data-component="EmphasisedText"><p>“这些国家为训练人工智能提供劳动力，在通往奇点[即机器比人类更聪明时]，或更现实的、无处不在的个人人工智能助手的竞赛中，他们不应被忽视或被抛在后面，”她说。</p></div><p>新加坡国立大学副校长陈西文教授则表示，全球在推进管理集体性行动问题上功能失调，要解决这些挑战，要么需要重新思考如何从结构上激励各方监管，要么需要等待一场危机，从而暴露出监管和协调的必要性。</p></div></div></div><div data-component="Dateline"><span data-component="Text">Posted<!-- --> </span>on <time data-component="Text">Wed 5 Jun 2024 at 7:11am</time></div></div></div></div>
